{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c487d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a778297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harry/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770922b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/harry/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0505ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/harry/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c010d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c69d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "711c7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../cord.db\")\n",
    "df = pd.read_sql_query(\"SELECT title, abstract, authors, body_text FROM cord19 \", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f69ec834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "      <td>Crouch, Erika C</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Clinical features of culture-proven Mycoplasma...   \n",
       "1  Nitric oxide: a pro-inflammatory mediator in l...   \n",
       "2    Surfactant protein-D and pulmonary host defense   \n",
       "3               Role of endothelin-1 in lung disease   \n",
       "4  Gene expression in epithelial cells in respons...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  OBJECTIVE: This retrospective chart review des...   \n",
       "1  Inflammatory diseases of the respiratory tract...   \n",
       "2  Surfactant protein-D (SP-D) participates in th...   \n",
       "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   \n",
       "4  Respiratory syncytial virus (RSV) and pneumoni...   \n",
       "\n",
       "                                             authors body_text  \n",
       "0                Madani, Tariq A; Al-Ghamdi, Aisha A            \n",
       "1  Vliet, Albert van der; Eiserich, Jason P; Cros...            \n",
       "2                                    Crouch, Erika C            \n",
       "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M            \n",
       "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...            "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df24ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# documents = df['abstract'].str.lower().apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5937a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "# documents_without_stop_words = []\n",
    "# for document in documents :\n",
    "#     documents_without_stop_words.append([word for word in document if word not in nltk_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d078d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# for i, document in enumerate(documents_without_stop_words) :\n",
    "#     documents_without_stop_words[i] = [wordnet_lemmatizer.lemmatize(word) for word in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4bb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "authors = df['authors']\n",
    "abstracts = df['abstract']\n",
    "body_texts = df['body_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403f45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'[A-Za-z]+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    " \n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in nltk_stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t.lower()) for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6cdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=preprocess_text, min_df=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da8f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tf_idf_fit = vectorizer.fit(titles + abstracts + body_texts)\n",
    "\n",
    "title_tf_idf = vectorizer.transform(titles)\n",
    "document_tf_idf = vectorizer.transform(abstracts + body_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0df3342b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>access</th>\n",
       "      <th>accompanied</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>...</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>year</th>\n",
       "      <th>yeast</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>young</th>\n",
       "      <th>zika</th>\n",
       "      <th>zikv</th>\n",
       "      <th>zoonotic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a  ability  able  absence  abundance  abundant  access  accompanied  \\\n",
       "0  0.0      0.0   0.0      0.0        0.0       0.0     0.0          0.0   \n",
       "1  0.0      0.0   0.0      0.0        0.0       0.0     0.0          0.0   \n",
       "2  0.0      0.0   0.0      0.0        0.0       0.0     0.0          0.0   \n",
       "3  0.0      0.0   0.0      0.0        0.0       0.0     0.0          0.0   \n",
       "4  0.0      0.0   0.0      0.0        0.0       0.0     0.0          0.0   \n",
       "\n",
       "   according  account  ...    x    y      year  yeast  yet  yield  young  \\\n",
       "0        0.0      0.0  ...  0.0  0.0  0.045744    0.0  0.0    0.0    0.0   \n",
       "1        0.0      0.0  ...  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   \n",
       "2        0.0      0.0  ...  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   \n",
       "3        0.0      0.0  ...  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   \n",
       "4        0.0      0.0  ...  0.0  0.0  0.000000    0.0  0.0    0.0    0.0   \n",
       "\n",
       "   zika  zikv  zoonotic  \n",
       "0   0.0   0.0       0.0  \n",
       "1   0.0   0.0       0.0  \n",
       "2   0.0   0.0       0.0  \n",
       "3   0.0   0.0       0.0  \n",
       "4   0.0   0.0       0.0  \n",
       "\n",
       "[5 rows x 2110 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = document_tf_idf.todense().tolist()\n",
    "tfidf = pd.DataFrame(dense, columns=feature_names)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5c67efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vectorizer.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84e711d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/title_tf_idf.pickle', 'wb') as file:\n",
    "    pickle.dump(title_tf_idf, file)\n",
    "    \n",
    "with open('../data/document_tf_idf.pickle', 'wb') as file:\n",
    "    pickle.dump(document_tf_idf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758480e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e291e0bbfcbaa0c01b86ae1ced984cafb447766c8d33fa2db84f5e188a4f964"
  },
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
